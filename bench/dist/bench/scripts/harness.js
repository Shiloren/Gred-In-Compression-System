import fs from 'fs';
import path from 'path';
import os from 'os';
import { generateTrendInt, generateVolatileInt, generateTrendIntLarge } from './datasets.js';
import { measure, measureSplit } from './metrics.js';
import { ZstdComparator } from './comparators.js';
// Import GICS - Adjust path if needed based on repo structure. 
// Assuming src/index.ts exports HybridWriter or similar.
// I will try to import from the compiled dist or src via tsx.
// For now, I'll assume the user has a GICS entry point.
// If I can't find it, I'll need to inspect.
// I saw 'benchmarks.ts' used: import { HybridWriter } from './src/index.js';
import { HybridWriter } from '../../src/index.js';
async function main() {
    const timestamp = new Date().toISOString();
    const results = [];
    // Environment
    const env = {
        cpu: os.cpus()[0].model,
        os: `${os.type()} ${os.release()}`,
        node: process.version
    };
    console.log(`Starting Benchmark Harness [${timestamp}]`);
    console.log(`Env: ${JSON.stringify(env)}`);
    // 1. Prepare Datasets
    const datasets = [
        generateTrendInt(100_000, 12345),
        generateVolatileInt(100_000, 12345),
        // generateTrendIntLarge(12345) // Warning: this is large (100MB+). Commented for speed unless requested.
        // User requested: "Nuevo dataset obligatorio: TS_TREND_INT_LARGE"
        // I will enable it but maybe scale it down slightly if node heap issues occur, 
        // or ensure I run with high memory. 5M rows is ~125MB JSON string.
        generateTrendIntLarge(12345)
    ];
    // 2. Prepare Comparators
    const zstd = new ZstdComparator();
    await zstd.init();
    // 3. Run Loop
    for (const ds of datasets) {
        console.log(`\nDataset: ${ds.name} (${ds.size_bytes} bytes)`);
        // --- GICS ---
        const gicsResult = await runGicsEncode(ds);
        results.push({
            timestamp_utc: timestamp,
            git_commit: 'HEAD', // TODO: get actual commit
            cpu_model: env.cpu,
            os_info: env.os,
            node_version: env.node,
            dataset: { name: ds.name, size: ds.size_bytes, checksum: ds.checksum },
            workload: 'BENCH-ENC-001',
            system: 'GICS',
            metrics: gicsResult,
            validity_flags: []
        });
        // --- GICS: Append Workload ---
        // Only run for smaller datasets or handle separate logic if needed, but running for all per requirements
        const gicsAppendResult = await runGicsAppend(ds, 5); // 5 chunks
        results.push({
            timestamp_utc: timestamp,
            git_commit: 'HEAD',
            cpu_model: env.cpu,
            os_info: env.os,
            node_version: env.node,
            dataset: { name: ds.name, size: ds.size_bytes * 5, checksum: 'N/A' },
            workload: 'BENCH-ENC-APPEND-001',
            system: 'GICS',
            metrics: gicsAppendResult,
            validity_flags: []
        });
        // --- ZSTD ---
        try {
            const zstdResult = await runComparatorEncode(ds, zstd);
            results.push({
                timestamp_utc: timestamp,
                git_commit: 'HEAD',
                cpu_model: env.cpu,
                os_info: env.os,
                node_version: env.node,
                dataset: { name: ds.name, size: ds.size_bytes, checksum: ds.checksum },
                workload: 'BENCH-ENC-001',
                system: 'BASELINE_ZSTD',
                metrics: zstdResult,
                validity_flags: []
            });
        }
        catch (err) {
            console.error(`Skipping ZSTD for ${ds.name}: ${err.message}`);
        }
    }
    // 4. Save
    const outFile = path.join(process.cwd(), 'bench/results', `run-${timestamp.replace(/:/g, '-')}.json`);
    fs.writeFileSync(outFile, JSON.stringify(results, null, 2));
    console.log(`\nSaved ${results.length} results to ${outFile}`);
}
async function runGicsEncode(ds) {
    // Measure construction vs run
    const measured = await measureSplit(async () => {
        return new HybridWriter();
    }, async (writer) => {
        for (const row of ds.data) {
            const itemMap = new Map();
            itemMap.set(1, { price: row.v, quantity: 1 });
            await writer.addSnapshot({ timestamp: row.t, items: itemMap });
        }
        return await writer.finish();
    });
    const output = measured.result;
    return {
        ...measured.metrics,
        output_bytes: output.length,
        ratio_x: ds.size_bytes / output.length
    };
}
async function runGicsAppend(ds, chunks) {
    const measured = await measureSplit(async () => {
        return new HybridWriter();
    }, async (writer) => {
        for (let c = 0; c < chunks; c++) {
            // Ingest full dataset as a "chunk"
            for (const row of ds.data) {
                const itemMap = new Map();
                itemMap.set(1, { price: row.v, quantity: 1 });
                // Shift timestamp to avoid overlap
                await writer.addSnapshot({ timestamp: row.t + (c * 1_000_000), items: itemMap });
            }
        }
        return await writer.finish();
    });
    const output = measured.result;
    return {
        ...measured.metrics,
        output_bytes: output.length,
        // Approximate ratio based on input size * chunks
        ratio_x: (ds.size_bytes * chunks) / output.length
    };
}
async function runComparatorEncode(ds, comp) {
    const inputBuf = Buffer.from(JSON.stringify(ds.data)); // Raw JSON input
    const measured = await measure(async () => {
        return await comp.compress(inputBuf);
    });
    const output = measured.result;
    return {
        ...measured.metrics,
        output_bytes: output.length,
        ratio_x: ds.size_bytes / output.length
    };
}
main().catch(err => {
    console.error(err);
    process.exit(1);
});
